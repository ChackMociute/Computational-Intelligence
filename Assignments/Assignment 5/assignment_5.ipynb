{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea717da-5328-4996-a7d3-f949d1b74dbd",
   "metadata": {
    "id": "QRBDdr0SEqpT"
   },
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa18eb1-e4be-412c-9cb3-b97b45da6315",
   "metadata": {
    "id": "8xd57TRzExEr"
   },
   "source": [
    "**Assignment 5: Neuroevolution: Neural Architecture**\n",
    "\n",
    "**Goal**: Get familiar with how neuroevolution is used for neural network architecture optimization by optimizing a neural network using neuroevolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459c67bc-d529-4e80-9293-db69f50f911e",
   "metadata": {
    "id": "Jxgc7c--P0GH"
   },
   "source": [
    "## 1 Understanding the problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3be99e-8181-4fbe-975d-4925a26f282f",
   "metadata": {
    "id": "oRteDLEPP3eX"
   },
   "source": [
    "*Problem description goes here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0c3584-e1fc-4a69-8402-84d9a28efc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d3b8055-3c67-48a4-909d-b8f7119502a0",
   "metadata": {
    "id": "Xm4e0Utl-30c"
   },
   "outputs": [],
   "source": [
    "class Dataset_:\n",
    "    class Digits(Dataset):\n",
    "        def __init__(self, mode, transforms):\n",
    "            self.digits = load_digits()\n",
    "            if mode == 'train':\n",
    "                self.data = self.digits.data[:1000].astype(np.float32)\n",
    "                self.targets = self.digits.target[:1000]\n",
    "            elif mode == 'val':\n",
    "                self.data = self.digits.data[1000:1350].astype(np.float32)\n",
    "                self.targets = self.digits.target[1000:1350]\n",
    "            else:\n",
    "                self.data = self.digits.data[1350:].astype(np.float32)\n",
    "                self.targets = self.digits.target[1350:]\n",
    "                \n",
    "            self.transforms = transforms\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            sample_x = self.data[idx].reshape(-1, 8, 8)\n",
    "            sample_y = self.targets[idx]\n",
    "            if self.transforms:\n",
    "                sample_x = self.transforms(sample_x)\n",
    "            return (sample_x, sample_y)\n",
    "    \n",
    "    def select_mode(self, mode, transforms=None):\n",
    "        return self.Digits(mode, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d51a9717-02b6-4339-8825-1d9f1d177553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhenotypeExpresser:\n",
    "    class ClassifierNeuralNet(nn.Module):\n",
    "        def __init__(self, classnet):\n",
    "            super(PhenotypeExpresser.ClassifierNeuralNet, self).__init__()\n",
    "            self.classnet = classnet\n",
    "            self.nll = nn.NLLLoss(reduction='none')\n",
    "\n",
    "        def classify(self, x):\n",
    "            # Return the indices with the highest probability which\n",
    "            # are equivalent to the class labels because the labels are digits\n",
    "            return torch.argmax(self.classnet(x), axis=1)\n",
    "\n",
    "        def forward(self, x, y, reduction='avg'):\n",
    "            # Calculate the negative log likelihood loss based on the target values y\n",
    "            # and how the classifier network classifies data x\n",
    "            loss = self.nll(self.classnet(x), y.type(torch.LongTensor))\n",
    "            if reduction == 'sum':\n",
    "                return loss.sum()\n",
    "            return loss.mean()\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activations = [nn.ReLU(), nn.Sigmoid(), nn.Tanh(), nn.Softplus(), nn.ELU()]\n",
    "        self.filters = [8, 16, 32]\n",
    "    \n",
    "    def get_conv_layer(self, filters, structure):\n",
    "        if structure: kernel, padding = 3, 1\n",
    "        else: kernel, padding = 5, 2\n",
    "        stride = 1\n",
    "        return nn.Conv2d(1, self.filters[filters], kernel_size=kernel, padding=padding, stride=stride)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_pool_layer(kernel, avg_pool):\n",
    "        return nn.AvgPool2d(kernel) if avg_pool else nn.MaxPool2d(kernel)\n",
    "    \n",
    "    def get_input_size(self, filters, structure, kernel_pool):\n",
    "        if structure: kernel, padding = 3, 1\n",
    "        else: kernel, padding = 5, 2\n",
    "        return int(self.filters[filters]*((8-kernel+2*padding+1)/kernel_pool)**2)\n",
    "    \n",
    "    def get_phenotypes(self, genotypes):\n",
    "        phenotypes = []\n",
    "        for genotype in genotypes:\n",
    "            classnet = nn.Sequential(\n",
    "                self.get_conv_layer(*genotype[:2]),\n",
    "                self.activations[genotype[2]],\n",
    "                self.get_pool_layer(*genotype[3:5]),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(self.get_input_size(*genotype[np.asarray([0,1,3])]), genotype[5]*10),\n",
    "                self.activations[genotype[2]],\n",
    "                nn.Linear(genotype[5]*10, 10),\n",
    "                nn.LogSoftmax(dim=1))\n",
    "            \n",
    "            phenotypes.append(self.ClassifierNeuralNet(classnet))\n",
    "        return np.asarray(phenotypes)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b325fdec-0386-46ef-a5c7-661c37b4d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, epochs=50, lr=1e-3, wd=1e-5):\n",
    "        self.PE = PhenotypeExpresser()\n",
    "        dataset = Dataset_()\n",
    "        [train, val, test] = [dataset.select_mode(mode) for mode in ['train', 'val', 'test']]\n",
    "        self.training_loader = DataLoader(train, batch_size=64, shuffle=True)\n",
    "        self.val_loader = DataLoader(val, batch_size=64, shuffle=False)\n",
    "        self.test_loader = DataLoader(test, batch_size=64, shuffle=False)\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "    \n",
    "    def test(self, model, test_loader):\n",
    "        model.eval()\n",
    "        loss_test = 0.\n",
    "        loss_error = 0.\n",
    "        N = 0.\n",
    "        \n",
    "        # start evaluation\n",
    "        for indx_batch, (test_batch, test_targets) in enumerate(test_loader):\n",
    "            loss_test_batch = model.forward(test_batch, test_targets, reduction='sum')\n",
    "            loss_test = loss_test + loss_test_batch.item()\n",
    "            # classification error\n",
    "            y_pred = model.classify(test_batch)\n",
    "            e = 1.*(y_pred == test_targets)\n",
    "            loss_error = loss_error + (1. - e).sum().item()\n",
    "            # the number of examples\n",
    "            N = N + test_batch.shape[0]\n",
    "            \n",
    "        loss_test = loss_test / N\n",
    "        loss_error = loss_error / N\n",
    "\n",
    "        return loss_test, loss_error\n",
    "    \n",
    "    def train(self, model, optimizer):\n",
    "        nll_val, error_val, beast_nll = [], [], 1000.\n",
    "\n",
    "        # Main training loop\n",
    "        for e in range(self.epochs):\n",
    "            model.train()\n",
    "            for indx_batch, (batch, targets) in enumerate(self.training_loader):\n",
    "                loss = model.forward(batch, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "\n",
    "            # Validation: Evaluate the model on the validation data\n",
    "            loss_e, error_e = self.test(model, self.val_loader)\n",
    "            nll_val.append(loss_e)\n",
    "            error_val.append(error_e)\n",
    "        \n",
    "        nll_val, error_val = np.asarray(nll_val), np.asarray(error_val)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def evaluate(self, x):\n",
    "        f = []\n",
    "        x = self.PE.get_phenotypes(x)\n",
    "        for model in x:\n",
    "            optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad == True],\n",
    "                                         lr=self.lr, weight_decay=self.wd) \n",
    "            model = self.train(model, optimizer)\n",
    "            _, error = self.test(model, self.test_loader)\n",
    "            f.append(error)\n",
    "            print(error)\n",
    "        return np.asarray(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c94ab1f8-e3e1-42b5-b5d6-c2ff07c0855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08053691275167785\n",
      "0.06487695749440715\n",
      "0.04697986577181208\n",
      "0.06711409395973154\n",
      "0.07606263982102908\n",
      "0.05592841163310962\n",
      "0.058165548098434\n",
      "0.06487695749440715\n",
      "0.06487695749440715\n",
      "0.07829977628635347\n",
      "0.09172259507829977\n",
      "0.06263982102908278\n",
      "0.06711409395973154\n",
      "0.07829977628635347\n",
      "0.06935123042505593\n",
      "0.06263982102908278\n",
      "0.07606263982102908\n",
      "0.10514541387024609\n",
      "0.08053691275167785\n",
      "0.09395973154362416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.08053691, 0.06487696, 0.04697987, 0.06711409, 0.07606264,\n",
       "       0.05592841, 0.05816555, 0.06487696, 0.06487696, 0.07829978,\n",
       "       0.0917226 , 0.06263982, 0.06711409, 0.07829978, 0.06935123,\n",
       "       0.06263982, 0.07606264, 0.10514541, 0.08053691, 0.09395973])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_generations = 50\n",
    "pop_size = 20\n",
    "bounds_min = np.asarray([0, 0, 0, 1, 0, 1])\n",
    "bounds_max = np.asarray([2, 1, 4, 2, 1, 10]) + 1\n",
    "\n",
    "\n",
    "x = np.random.randint(low=bounds_min, high=bounds_max, size=(pop_size, 6))\n",
    "\n",
    "# PE = PhenotypeExpresser()\n",
    "# print(PE.get_phenotypes(x))\n",
    "\n",
    "E = Evaluator()\n",
    "\n",
    "E.evaluate(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f56da-d425-4fa3-88d8-1283fa724a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EA:\n",
    "    def __init__(self, evaluator, pop_size, std=20, parents=None, bounds_min=[], bounds_max=[]):\n",
    "        self.evaluator = evaluator\n",
    "        self.pop_size = pop_size\n",
    "        self.std = std\n",
    "        self.parents = parents if parents is not None else pop_size\n",
    "        self.bounds_min = np.asarray(bounds_min)\n",
    "        self.bounds_max = np.asarray(bounds_max)\n",
    "    \n",
    "    # Select and return a random number of parents equal to parents hyperparameter\n",
    "    def parent_selection(self, x_old, f_old):\n",
    "        ind = np.random.choice(self.pop_size, self.parents, replace=False)\n",
    "        return x_old[ind], f_old[ind]\n",
    "    \n",
    "    # Takes features from best parents and inserts them into the worst ones\n",
    "    def recombination(self, x_parents, f_parents):\n",
    "        return x_parents\n",
    "    \n",
    "    # Add some noise from a multivariate normal distribution to every child\n",
    "    def mutation(self, x_children):\n",
    "        scale = self.bounds_max - self.bounds_min\n",
    "        scale /= np.max(scale) # Normalize the scale\n",
    "        mutation = np.random.multivariate_normal(\n",
    "            np.zeros(4), np.diag(scale)*self.std,\n",
    "            size=x_children.shape[0])\n",
    "\n",
    "        return np.clip(x_children + mutation, self.bounds_min, self.bounds_max)\n",
    "    \n",
    "    # Select 80% of survivors based on the round robin tournament selection method\n",
    "    # The other 20% are selected randomly to help escape local optima\n",
    "    def survivor_selection(self, x_old, x_children, f_old, f_children):\n",
    "        x = np.concatenate([x_old, x_children])\n",
    "        f = np.concatenate([f_old, f_children])\n",
    "        scores = self.__winning_scores(f)\n",
    "        \n",
    "        ind = np.argsort(scores)\n",
    "        x = x[ind]\n",
    "        f = f[ind]\n",
    "        \n",
    "        threshold = int(np.ceil(self.pop_size*0.8))\n",
    "        ind = np.random.choice(range(threshold, x.shape[0]), int(0.2*self.pop_size), replace=False)\n",
    "        return np.concatenate([x[:threshold], x[ind]]), np.concatenate([f[:threshold], f[ind]])\n",
    "    \n",
    "    # For every individual, finds the number of wins against random opponents\n",
    "    def __winning_scores(self, f):\n",
    "        scores = []\n",
    "        for f_i in f:\n",
    "            ind = np.random.choice(x.shape[0], int(np.ceil(self.pop_size/10)), replace=False)\n",
    "            scores.append(sum([f_i >= i for i in f[ind]]))\n",
    "        \n",
    "        return np.asarray(scores)\n",
    "\n",
    "    # Evaluation step: DO NOT REMOVE!\n",
    "    def evaluate(self, x):\n",
    "        return self.evaluator.evaluate(x)\n",
    "  \n",
    "    def step(self, x_old, f_old):\n",
    "        x_parents, f_parents = self.parent_selection(x_old, f_old)\n",
    "        x_children = self.recombination(x_parents, f_parents)\n",
    "        x_children = self.mutation(x_children)\n",
    "        f_children = self.evaluate(x_children)\n",
    "        x, f = self.survivor_selection(x_old, x_children, f_old, f_children)\n",
    "                       \n",
    "        return x, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f3f456-ea5c-445c-88e6-499234f3e86b",
   "metadata": {
    "id": "Vtv_pAkmOrS3"
   },
   "outputs": [],
   "source": [
    "#=========\n",
    "# GRADING:\n",
    "# 0 \n",
    "# 0.5 pt if code works but it is explained badly\n",
    "# 1.0 pt if code works and it is explained well\n",
    "#=========\n",
    "# Implement a neural network (NN) classifier. \n",
    "class ClassifierNeuralNet(nn.Module):\n",
    "    def __init__(self, classnet):\n",
    "        super(ClassifierNeuralNet, self).__init__()\n",
    "        # We provide a sequential module with layers and activations\n",
    "        self.classnet = classnet\n",
    "        # The loss function (the negative log-likelihood)\n",
    "        self.nll = nn.NLLLoss(reduction='none') #it requires log-softmax as input!!\n",
    "\n",
    "    # This function classifies an image x to a class.\n",
    "    # The output must be a class label (long).\n",
    "    def classify(self, x):\n",
    "        # Return the indices with the highest probability which\n",
    "        # are equivalent to the class labels because the labels are digits\n",
    "        return torch.argmax(self.classnet(x), axis=1)\n",
    "\n",
    "    # This function is crucial for a module in PyTorch.\n",
    "    # In our framework, this class outputs a value of the loss function.\n",
    "    def forward(self, x, y, reduction='avg'):\n",
    "        # Calculate the negative log likelihood loss based on the target values y\n",
    "        # and how the classifier network classifies data x\n",
    "        loss = self.nll(self.classnet(x), y.type(torch.LongTensor))\n",
    "        \n",
    "        if reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
