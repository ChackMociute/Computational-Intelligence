{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRBDdr0SEqpT"
      },
      "source": [
        "# Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xd57TRzExEr"
      },
      "source": [
        "**Assignment 1: Optimization**\n",
        "\n",
        "**Goal**: ​Get familiar with gradient-based and derivative-free optimization by implementing these methods and applying them to a given function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHNgWB1iFDu5"
      },
      "source": [
        "In this assignment we are going to learn about **gradient-based** (GD) optimization methods and **derivative-free optimization** (DFO) methods. The goal is to implement these methods (one from each group) and analyze their behavior. Importantly, we aim at noticing differences between these two groups of methods.\n",
        "\n",
        "Here, we are interested in ​minimizing​ the following function:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x})=x_{1}^{2}+2 x_{2}^{2}-0.3 \\cos \\left(3 \\pi x_{1}\\right)-0.4 \\cos \\left(4 \\pi x_{2}\\right)+0.7\n",
        "$$\n",
        "\n",
        "in the domain $\\mathbf{x} = (x_1, x_2) \\in [-100, 100]^{2}$ (i.e., $x_1 \\in [-100, 100]$, $x_2 \\in [-100, 100]$).\n",
        "\n",
        "In this assignemnt, you are asked to implement:\n",
        "1. The gradient-descent algorithm.\n",
        "2. A chosen derivative-free algorithm. *You are free to choose a method.*\n",
        "\n",
        "After implementing both methods, please run experiments and compare both methods. Please find a more detailed description below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxgc7c--P0GH"
      },
      "source": [
        "## 1. Understanding the objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRteDLEPP3eX"
      },
      "source": [
        "Please run the code below and visualize the objective function. Please try to understand the objective function, what is the optimum (you can do it by inspecting the plot).\n",
        "\n",
        "If any code line is unclear to you, please read on that in numpy or matplotlib docs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4wCnPRz-MaE"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzHAz9R5-dfQ"
      },
      "source": [
        "# PLEASE DO NOT REMOVE!\n",
        "# The objective function.\n",
        "def f(x):\n",
        "    return x[:,0]**2 + 2*x[:,1]**2 -0.3*np.cos(3.*np.pi*x[:,0])-0.4*np.cos(4.*np.pi*x[:,1])+0.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs3gdKfAC25-"
      },
      "source": [
        "# PLEASE DO NOT REMOVE!\n",
        "# Calculating the objective for visualization.\n",
        "def calculate_f(x1, x2):\n",
        "  f_x = []\n",
        "  for i in range(len(x1)):\n",
        "    for j in range(len(x2)):\n",
        "      f_x.append(f(np.asarray([[x1[i], x2[j]]])))\n",
        "    \n",
        "  return np.asarray(f_x).reshape(len(x1), len(x2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdDjKDRI-u76"
      },
      "source": [
        "# PLEASE DO NOT REMOVE!\n",
        "# Define coordinates\n",
        "x1 = np.linspace(-100., 100., 400)\n",
        "x2 = np.linspace(-100., 100., 400)\n",
        "\n",
        "# Calculate the objective\n",
        "f_x = calculate_f(x1, x2).reshape(len(x1), len(x2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtAKEWqt_Mkt"
      },
      "source": [
        "# PLEASE DO NOT REMOVE!\n",
        "# Plot the objective\n",
        "plt.contourf(x1, x2, f_x, 100, cmap='hot')\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgc_GFOyRBEi"
      },
      "source": [
        "## 2. The gradient-descent algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDVf1vyORGUB"
      },
      "source": [
        "First, you are asked to implement the gradient descent (GD) algorithm. Please take a look at the class below and fill in the missing parts.\n",
        "\n",
        "NOTE: Please pay attention to the inputs and outputs of each function.\n",
        "\n",
        "NOTE: To implement the GD algorithm, we need a gradient with respect to $\\mathbf{x}$ of the given function. Please calculate it on a paper and provide the solution below. Then, implement it in an appropriate function that will be further passed to the GD class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76QTS-BHRlch"
      },
      "source": [
        "**Question 1 (0-1pt):** What is the gradient of the function $f(\\mathbf{x})$?\n",
        "\n",
        "**Answer:**\n",
        "\\begin{align*}\n",
        "\\nabla_{\\mathbf{x}_1} f(\\mathbf{x}) &= ? \\\\\n",
        "\\nabla_{\\mathbf{x}_2} f(\\mathbf{x}) &= ? \\\\\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq5nJgLXT4v4"
      },
      "source": [
        "#=========\n",
        "# GRADING:\n",
        "# 0\n",
        "# 0.5pt - if properly implemented and commented well\n",
        "#=========\n",
        "# Implement the gradient for the considered f(x).\n",
        "def grad(x):\n",
        "  #------\n",
        "  # PLEASE FILL IN:\n",
        "  # ...\n",
        "  # grad = ...\n",
        "  #------\n",
        "  return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtv_pAkmOrS3"
      },
      "source": [
        "#=========\n",
        "# GRADING:\n",
        "# 0\n",
        "# 0.5pt if properly implemented and commented well\n",
        "#=========\n",
        "# Implement the gradient descent (GD) optimization algorithm. \n",
        "# It is equivalent to implementing the step function.\n",
        "class GradientDescent(object):\n",
        "  def __init__(self, grad, step_size=0.1):\n",
        "    self.grad = grad\n",
        "    self.step_size = step_size    \n",
        "\n",
        "  def step(self, x_old):\n",
        "    #------\n",
        "    # PLEASE FILL IN:\n",
        "    # ...\n",
        "    # x_new = ...\n",
        "    #------\n",
        "    return x_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNCsf37T4gcp"
      },
      "source": [
        "# PLEASE DO NOT REMOVE!\n",
        "# An auxiliary function for plotting.\n",
        "def plot_optimization_process(ax, optimizer, title):\n",
        "  # Plot the objective function\n",
        "  ax.contourf(x1, x2, f_x, 100, cmap='hot')\n",
        "\n",
        "  # Init the solution\n",
        "  x = np.asarray([[90., -90.]])\n",
        "  x_opt = x\n",
        "  # Run the optimization algorithm\n",
        "  for i in range(num_epochs):\n",
        "    x = optimizer.step(x)\n",
        "    x_opt = np.concatenate((x_opt, x), 0)\n",
        "\n",
        "  ax.plot(x_opt[:,0], x_opt[:,1], linewidth=3.)\n",
        "  ax.set_title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6meSu2l5gYw"
      },
      "source": [
        "# PLEASE DO NOT REMOVE!\n",
        "# This piece of code serves for the analysis.\n",
        "# Running the GD algorithm with different step sizes\n",
        "num_epochs = 20 # the number of epochs\n",
        "step_sizes = [0.01, 0.05, 0.1, 0.25, 0.4, 0.5] # the step sizes\n",
        "\n",
        "# plotting the convergence of the GD\n",
        "fig_gd, axs = plt.subplots(1,len(step_sizes),figsize=(15, 2))\n",
        "fig_gd.tight_layout()\n",
        "\n",
        "for i in range(len(step_sizes)):\n",
        "  # take the step size\n",
        "  step_size = step_sizes[i]\n",
        "  # init the GD\n",
        "  gd = GradientDescent(grad, step_size=step_size)\n",
        "  # plot the convergence\n",
        "  plot_optimization_process(axs[i], optimizer=gd, title='Step size ' + str(gd.step_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KBcdBUEDRLF"
      },
      "source": [
        "**Question 2 (0-0.5pt)**: Please analyze the plots above and comment on the behavior of the gradient-descent for different values of the step size.\n",
        "\n",
        "**Answer**: PLEASE FILL IN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR_twjU-vJLz"
      },
      "source": [
        "\n",
        "**Question 3 (0-0.5pt)**: Can we do something about the step size equal 0.01? What about the step size equal 0.5?\n",
        "\n",
        "**Answer**: PLEASE FILL IN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAMgPBGV_oJu"
      },
      "source": [
        "## 3. The derivative-free optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inAKXR7w_svO"
      },
      "source": [
        "In the second part of this assignment, you are asked to implement a derivative-free optimziation (DFO) algorithm. Please notice that you are free to choose any DFO method you wish.\n",
        "Moreover, you are encouraged to be as imaginative as possible! Do you have an idea for a new method or combine multiple methods? Great! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_-dEK1uCO3n"
      },
      "source": [
        "**Question 4 (0-0.5-1-1.5-2-2.5-3pt)**: Please provide a description (a pseudocode) of your DFO method here.\n",
        "\n",
        "*NOTE (grading): The more complex the method, the higher the score! Please keep it in mind during developing your algorithm. TAs will also check whether the pseudocode is correct.*\n",
        "\n",
        "**Answer:** PLEASE FILL IN\n",
        "\n",
        "*Input:* PLEASE FILL IN\n",
        "\n",
        "1. PLEASE FILL IN\n",
        "2. ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn_mYv2c6T4-"
      },
      "source": [
        "#=========\n",
        "# GRADING: 0-0.5-1-1.5-2pt\n",
        "# 0\n",
        "# 0.5pt the code works but it is very messy and unclear\n",
        "# 1.0pt the code works but it is messy and badly commented\n",
        "# 1.5pt the code works but it is hard to follow in some places\n",
        "# 2.0pt the code works and it is fully understandable\n",
        "#=========\n",
        "# Implement a derivative-free optimization (DFO) algorithm. \n",
        "# REMARK: during the init, you are supposed to pass the obj_fun and other objects that are necessary in your method.\n",
        "class DFO(object):\n",
        "  def __init__(self, obj_fun, step_size):\n",
        "    self.obj_fun = obj_fun\n",
        "    # PLEASE FILL IN: You will need some other variables\n",
        "    #...\n",
        "  \n",
        "  ## PLEASE FILL IN IF NECESSARY\n",
        "  ## Please remember that for the DFO you may need extra functions.\n",
        "  #def ...\n",
        "\n",
        "  # This function MUST be implemented.\n",
        "  # No additional arguments here!\n",
        "  def step(self, x_old):\n",
        "    ## PLEASE FILL IN.\n",
        "    #...\n",
        "    #x_new = ...\n",
        "    return x_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMwwd_6zEZF6"
      },
      "source": [
        "# PLEASE DO NOT REMOVE!\n",
        "# Running the DFO algorithm with different step sizes\n",
        "num_epochs = 20 # the number of epochs (you may change it!)\n",
        "\n",
        "## PLEASE FILL IN\n",
        "## Here all hyperparameters go.\n",
        "## Please analyze at least one hyperparameter in a similar manner to the \n",
        "## step size in the GD algorithm.\n",
        "# ...\n",
        "\n",
        "## plotting the convergence of the DFO\n",
        "## Please uncomment the two lines below, but please provide the number of axes (replace HERE appriopriately)\n",
        "# fig_dfo, axs = plt.subplots(1, HERE, figsize=(15, 2))\n",
        "# fig_dfo.tight_layout()\n",
        "\n",
        "# the for-loop should go over (at least one) parameter(s) (replace HERE appriopriately)\n",
        "# and uncomment the line below\n",
        "# for i in range(HERE):\n",
        "\n",
        "  ## PLEASE FILL IN\n",
        "  # ...\n",
        "  # dfo = DFO(f, ...)\n",
        "  # plot the convergence\n",
        "  # please change the title accordingly!\n",
        "  plot_optimization_process(axs[i], optimizer=dfo, title='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4omq4-84e3mX"
      },
      "source": [
        "**Question 5 (0-0.5-1pt)** Please comment on the behavior of your DFO algorithm. What are the strong points? What are the (potential) weak points? During working on the algorithm, what kind of problems did you encounter?\n",
        "\n",
        "**Answer:** PLEASE FILL IN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFX-DzH9ftPg"
      },
      "source": [
        "## 4. Final remarks: GD vs. DFO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-TFtGdZfz3a"
      },
      "source": [
        "Eventually, please answer the following last question that will allow you to conclude the assignment draw conclusions.\n",
        "\n",
        "**Question 6 (0-0.5pt)**: What are differences between the two approaches?\n",
        "\n",
        "**Answer**: PLEASE FILL IN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rlm5WVT_Uhd"
      },
      "source": [
        "**Question 7 (0-0.5)**: Which of the is easier to apply? Why? In what situations? Which of them is easier to implement in general?\n",
        "\n",
        "**Answer**: PLEASE FILL IN"
      ]
    }
  ]
}