{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea717da-5328-4996-a7d3-f949d1b74dbd",
   "metadata": {
    "id": "QRBDdr0SEqpT"
   },
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa18eb1-e4be-412c-9cb3-b97b45da6315",
   "metadata": {
    "id": "8xd57TRzExEr"
   },
   "source": [
    "**Assignment 5: Neuroevolution: Neural Architecture**\n",
    "\n",
    "**Goal**: Get familiar with how neuroevolution is used for neural network architecture optimization by optimizing a neural network using neuroevolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459c67bc-d529-4e80-9293-db69f50f911e",
   "metadata": {
    "id": "Jxgc7c--P0GH"
   },
   "source": [
    "## 1 Understanding the problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3be99e-8181-4fbe-975d-4925a26f282f",
   "metadata": {
    "id": "oRteDLEPP3eX"
   },
   "source": [
    "*Problem description goes here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b0c3584-e1fc-4a69-8402-84d9a28efc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3b8055-3c67-48a4-909d-b8f7119502a0",
   "metadata": {
    "id": "Xm4e0Utl-30c"
   },
   "outputs": [],
   "source": [
    "class Dataset_:\n",
    "    class Digits(Dataset):\n",
    "        def __init__(self, mode, transforms):\n",
    "            self.digits = load_digits()\n",
    "            if mode == 'train':\n",
    "                self.data = self.digits.data[:1000].astype(np.float32)\n",
    "                self.targets = self.digits.target[:1000]\n",
    "            elif mode == 'val':\n",
    "                self.data = self.digits.data[1000:1350].astype(np.float32)\n",
    "                self.targets = self.digits.target[1000:1350]\n",
    "            else:\n",
    "                self.data = self.digits.data[1350:].astype(np.float32)\n",
    "                self.targets = self.digits.target[1350:]\n",
    "                \n",
    "            self.transforms = transforms\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            sample_x = self.data[idx].reshape(-1, 8, 8)\n",
    "            sample_y = self.targets[idx]\n",
    "            if self.transforms:\n",
    "                sample_x = self.transforms(sample_x)\n",
    "            return (sample_x, sample_y)\n",
    "    \n",
    "    def select_mode(self, mode, transforms=None):\n",
    "        return self.Digits(mode, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d51a9717-02b6-4339-8825-1d9f1d177553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhenotypeExpresser:\n",
    "    class ClassifierNeuralNet(nn.Module):\n",
    "        def __init__(self, classnet):\n",
    "            super(PhenotypeExpresser.ClassifierNeuralNet, self).__init__()\n",
    "            self.classnet = classnet\n",
    "            self.nll = nn.NLLLoss(reduction='none')\n",
    "\n",
    "        def classify(self, x):\n",
    "            # Return the indices with the highest probability which\n",
    "            # are equivalent to the class labels because the labels are digits\n",
    "            return torch.argmax(self.classnet(x), axis=1)\n",
    "\n",
    "        def forward(self, x, y, reduction='avg'):\n",
    "            # Calculate the negative log likelihood loss based on the target values y\n",
    "            # and how the classifier network classifies data x\n",
    "            loss = self.nll(self.classnet(x), y.type(torch.LongTensor))\n",
    "            if reduction == 'sum':\n",
    "                return loss.sum()\n",
    "            return loss.mean()\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._activations = [nn.ReLU(), nn.Sigmoid(), nn.Tanh(), nn.Softplus(), nn.ELU()]\n",
    "        self._filters = [8, 16, 32]\n",
    "    \n",
    "    def __get_conv_layer(self, filters, structure):\n",
    "        if structure: kernel, padding = 3, 1\n",
    "        else: kernel, padding = 5, 2\n",
    "        stride = 1\n",
    "        return nn.Conv2d(1, self._filters[filters], kernel_size=kernel, padding=padding, stride=stride)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_pool_layer(kernel, avg_pool):\n",
    "        kernel += 1\n",
    "        return nn.AvgPool2d(kernel) if avg_pool else nn.MaxPool2d(kernel)\n",
    "    \n",
    "    def __get_input_size(self, filters, structure, kernel_pool):\n",
    "        if structure: kernel, padding = 3, 1\n",
    "        else: kernel, padding = 5, 2\n",
    "        kernel_pool += 1\n",
    "        return int(self._filters[filters]*((8-kernel+2*padding+1)/kernel_pool)**2)\n",
    "    \n",
    "    def get_phenotypes(self, genotypes):\n",
    "        phenotypes = []\n",
    "        for genotype in genotypes:\n",
    "            classnet = nn.Sequential(\n",
    "                self.__get_conv_layer(*genotype[:2]),\n",
    "                self._activations[genotype[2]],\n",
    "                self.__get_pool_layer(*genotype[3:5]),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(self.__get_input_size(*genotype[np.asarray([0,1,3])]), genotype[5]*10),\n",
    "                self._activations[genotype[2]],\n",
    "                nn.Linear(genotype[5]*10, 10),\n",
    "                nn.LogSoftmax(dim=1))\n",
    "            \n",
    "            phenotypes.append(self.ClassifierNeuralNet(classnet))\n",
    "        return np.asarray(phenotypes)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b325fdec-0386-46ef-a5c7-661c37b4d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, epochs=30, lr=1e-3, wd=1e-5):\n",
    "        self.PE = PhenotypeExpresser()\n",
    "        dataset = Dataset_()\n",
    "        [train, val, test] = [dataset.select_mode(mode) for mode in ['train', 'val', 'test']]\n",
    "        self.training_loader = DataLoader(train, batch_size=64, shuffle=True)\n",
    "        self.val_loader = DataLoader(val, batch_size=64, shuffle=False)\n",
    "        self.test_loader = DataLoader(test, batch_size=64, shuffle=False)\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "    \n",
    "    def test(self, model, test_loader):\n",
    "        model.eval()\n",
    "        loss_test = 0.\n",
    "        loss_error = 0.\n",
    "        N = 0.\n",
    "        \n",
    "        # start evaluation\n",
    "        for indx_batch, (test_batch, test_targets) in enumerate(test_loader):\n",
    "            loss_test_batch = model.forward(test_batch, test_targets, reduction='sum')\n",
    "            loss_test = loss_test + loss_test_batch.item()\n",
    "            # classification error\n",
    "            y_pred = model.classify(test_batch)\n",
    "            e = 1.*(y_pred == test_targets)\n",
    "            loss_error = loss_error + (1. - e).sum().item()\n",
    "            # the number of examples\n",
    "            N = N + test_batch.shape[0]\n",
    "            \n",
    "        loss_test = loss_test / N\n",
    "        loss_error = loss_error / N\n",
    "\n",
    "        return loss_test, loss_error\n",
    "    \n",
    "    def train(self, model, optimizer):\n",
    "        nll_val, error_val, beast_nll = [], [], 1000.\n",
    "\n",
    "        # Main training loop\n",
    "        for e in range(self.epochs):\n",
    "            model.train()\n",
    "            for indx_batch, (batch, targets) in enumerate(self.training_loader):\n",
    "                loss = model.forward(batch, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "\n",
    "            # Validation: Evaluate the model on the validation data\n",
    "            loss_e, error_e = self.test(model, self.val_loader)\n",
    "            nll_val.append(loss_e)\n",
    "            error_val.append(error_e)\n",
    "        \n",
    "        nll_val, error_val = np.asarray(nll_val), np.asarray(error_val)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def evaluate(self, x):\n",
    "        f = []\n",
    "        x = self.PE.get_phenotypes(x)\n",
    "        for model in x:\n",
    "            optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                                         lr=self.lr, weight_decay=self.wd) \n",
    "            model = self.train(model, optimizer)\n",
    "            _, error = self.test(model, self.test_loader)\n",
    "            f.append(error)\n",
    "        return np.asarray(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ee0f56da-d425-4fa3-88d8-1283fa724a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EA:\n",
    "    def __init__(self, evaluator, pop_size, bounds_min, bounds_max, std=1, parents=None):\n",
    "        self.evaluator = evaluator\n",
    "        self.pop_size = pop_size\n",
    "        self.std = std\n",
    "        self.parents = parents if parents is not None else pop_size\n",
    "        self.bounds_min = bounds_min\n",
    "        self.bounds_max = bounds_max\n",
    "    \n",
    "    # Select and return a random number of parents equal to parents hyperparameter\n",
    "    def parent_selection(self, x_old, f_old):\n",
    "        ind = np.random.choice(self.pop_size, self.parents, replace=False)\n",
    "        return x_old, f_old#x_old[ind], f_old[ind]\n",
    "    \n",
    "    # Takes features from best parents and inserts them into the worst ones\n",
    "    def recombination(self, x_parents, f_parents):\n",
    "        return x_parents\n",
    "    \n",
    "    # Add some noise from a multivariate normal distribution to every child\n",
    "    def mutation(self, x_children):\n",
    "        binary_mutation = np.random.binomial(1, 0.2, (x_children.shape[0], 3))\n",
    "        integer_mutation = np.random.binomial(1, 0.2, (x_children.shape[0], 3)) \\\n",
    "                           * np.round(np.random.normal(0, self.std, size=(x_children.shape[0], 3)))\n",
    "        \n",
    "        x_children[:,np.array([1, 3, 4])] = (x_children[:,np.array([1, 3, 4])] + binary_mutation)%2 # Flipping bits\n",
    "        x_children[:,np.array([0, 2, 5])] = x_children[:,np.array([0, 2, 5])] + integer_mutation # Creep mutation\n",
    "\n",
    "        return np.clip(x_children, self.bounds_min, self.bounds_max)\n",
    "    \n",
    "    # Select 80% of survivors based on the round robin tournament selection method\n",
    "    # The other 20% are selected randomly to help escape local optima\n",
    "    def survivor_selection(self, x_old, x_children, f_old, f_children):\n",
    "        x = np.concatenate([x_old, x_children])\n",
    "        f = np.concatenate([f_old, f_children])\n",
    "        #scores = self.__winning_scores(f)\n",
    "        \n",
    "        ind = np.argsort(f)\n",
    "        x = x[ind]\n",
    "        f = f[ind]\n",
    "        \n",
    "        return x[:self.pop_size], f[self.pop_size]\n",
    "        #threshold = int(np.ceil(self.pop_size*0.8))\n",
    "        #ind = np.random.choice(range(threshold, x.shape[0]), int(0.2*self.pop_size), replace=False)\n",
    "        #return np.concatenate([x[:threshold], x[ind]]), np.concatenate([f[:threshold], f[ind]])\n",
    "    \n",
    "    # For every individual, finds the number of wins against random opponents\n",
    "    def __winning_scores(self, f):\n",
    "        scores = []\n",
    "        for f_i in f:\n",
    "            ind = np.random.choice(x.shape[0], int(np.ceil(self.pop_size/10)), replace=False)\n",
    "            scores.append(sum([f_i >= i for i in f[ind]]))\n",
    "        \n",
    "        return np.asarray(scores)\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        return self.evaluator.evaluate(x)\n",
    "  \n",
    "    def step(self, x_old, f_old):\n",
    "        x_parents, f_parents = self.parent_selection(x_old, f_old)\n",
    "        x_children = self.recombination(x_parents, f_parents)\n",
    "        x_children = self.mutation(x_children)\n",
    "        f_children = self.evaluate(x_children)\n",
    "        x, f = self.survivor_selection(x_old, x_children, f_old, f_children)\n",
    "                       \n",
    "        return x, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c94ab1f8-e3e1-42b5-b5d6-c2ff07c0855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 0, best fitness: 0.06\n",
      "Generation: 1, best fitness: 0.08\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [197]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(num_generations \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeneration: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, best fitness: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i, f\u001b[38;5;241m.\u001b[39mmin()))\n\u001b[1;32m---> 22\u001b[0m x, f \u001b[38;5;241m=\u001b[39m \u001b[43mea\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m populations\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m f_best[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "Input \u001b[1;32mIn [196]\u001b[0m, in \u001b[0;36mEA.step\u001b[1;34m(self, x_old, f_old)\u001b[0m\n\u001b[0;32m     61\u001b[0m x_children \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmutation(x_children)\n\u001b[0;32m     62\u001b[0m f_children \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(x_children)\n\u001b[1;32m---> 63\u001b[0m x, f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurvivor_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_old\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_children\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_old\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, f\n",
      "Input \u001b[1;32mIn [196]\u001b[0m, in \u001b[0;36mEA.survivor_selection\u001b[1;34m(self, x_old, x_children, f_old, f_children)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msurvivor_selection\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_old, x_children, f_old, f_children):\n\u001b[0;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([x_old, x_children])\n\u001b[1;32m---> 34\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf_old\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_children\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m#scores = self.__winning_scores(f)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     ind \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(f)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "num_generations = 10\n",
    "pop_size = 20\n",
    "bounds_min = np.asarray([0, 0, 0, 0, 0, 1])\n",
    "bounds_max = np.asarray([2, 1, 4, 1, 1, 10])\n",
    "\n",
    "\n",
    "e = Evaluator()\n",
    "ea = EA(e, pop_size, bounds_min, bounds_max)\n",
    "\n",
    "\n",
    "x = np.random.randint(low=bounds_min, high=bounds_max+1, size=(pop_size, 6))\n",
    "f = e.evaluate(x)\n",
    "\n",
    "populations = []\n",
    "populations.append(x)\n",
    "f_best = [f.min()]\n",
    "\n",
    "# Run the EA.\n",
    "for i in range(num_generations):\n",
    "    if i % int(num_generations * 0.1) == 0:\n",
    "        print('Generation: {}, best fitness: {:.2f}'.format(i, f.min()))\n",
    "    x, f = ea.step(x, f)\n",
    "    populations.append(x)\n",
    "    if f.min() < f_best[-1]:\n",
    "        f_best.append(f.min())\n",
    "    else:\n",
    "        f_best.append(f_best[-1])\n",
    "print('FINISHED!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
